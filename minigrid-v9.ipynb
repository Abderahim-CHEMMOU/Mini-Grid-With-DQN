{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium minigrid imageio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # ✅ Pour F.smooth_l1_loss\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque, namedtuple\nimport gymnasium as gym\nfrom minigrid.envs.empty import EmptyEnv\nfrom minigrid.wrappers import RGBImgObsWrapper\nimport imageio\nimport os\nimport time\nimport random","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Autoriser les types numpy pour charger le modèle\ntorch.serialization.add_safe_globals([np.core.multiarray.scalar, np.dtype])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Architecture du réseau\nclass EnhancedDQN(nn.Module):\n    def __init__(self, input_shape, n_actions):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((4, 4)),\n            nn.Flatten()\n        )\n        with torch.no_grad():\n            dummy = torch.zeros(1, *input_shape)\n            conv_out = self.conv(dummy).shape[1]\n        self.fc = nn.Sequential(\n            nn.Linear(conv_out, 256),\n            nn.ReLU(),\n            nn.Linear(256, n_actions)\n        )\n    def forward(self, x):\n        return self.fc(self.conv(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Environnement personnalisé avec positions aléatoires\nclass RandomGoalEmptyEnv(EmptyEnv):\n    def __init__(self, size=8):\n        super().__init__(size=size)\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self._gen_grid(self.width, self.height)\n        self.agent_pos = self._empty_cell()\n        self._goal_pos = self._empty_cell()\n        return self.gen_obs(), {}\n    def _empty_cell(self):\n        while True:\n            pos = self.np_random.integers(0, self.width, size=2)\n            if self.grid.get(*pos) is None and tuple(pos) != tuple(self.agent_pos):\n                return tuple(pos)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prétraitement des observations\ndef preprocess_observation(obs):\n    state = obs['image'].transpose(2, 0, 1).astype(np.float32) / 255.0\n    return (state - 0.5) / 0.5  # Normalisation à [-1, 1]\n\n# Structure de données pour le buffer de replay\nTransition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\nclass ReplayBuffer(deque):\n    def __init__(self, capacity):\n        super().__init__([], maxlen=capacity)\n    def push(self, *args):\n        self.append(Transition(*args))\n    def sample(self, batch_size):\n        return random.sample(list(self), batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sélection d'action avec epsilon-greedy\ndef select_action(state, policy_net, device, steps_done, n_actions, eval_mode=False):\n    eps = EPS_END + (EPS_START - EPS_END) * np.exp(-steps_done / EPS_DECAY)\n    if eval_mode or random.random() > eps:\n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n            return policy_net(state_t).argmax().item(), eps\n    else:\n        return random.randint(0, n_actions-1), eps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize(policy_net, target_net, buffer, optimizer, device):\n    if len(buffer) < BATCH_SIZE:\n        return 0.0\n    transitions = buffer.sample(BATCH_SIZE)\n    batch = Transition(*zip(*transitions))\n    states = torch.FloatTensor(np.array(batch.state)).to(device)\n    actions = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n    rewards = torch.FloatTensor(batch.reward).to(device)\n    dones = torch.BoolTensor(batch.done).to(device)\n    non_final_mask = ~dones\n    non_final_next_states = torch.FloatTensor(\n        np.array([s for s, d in zip(batch.next_state, batch.done) if not d])\n    ).to(device)\n    current_q = policy_net(states).gather(1, actions)\n    next_q_values = torch.zeros(BATCH_SIZE, device=device)\n    if len(non_final_next_states) > 0:\n        with torch.no_grad():\n            next_actions = policy_net(non_final_next_states).argmax(1, keepdim=True)\n            next_q_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_actions).squeeze()\n    target_values = rewards + GAMMA * next_q_values * (~dones).float()\n    loss = F.smooth_l1_loss(current_q, target_values.unsqueeze(1))  # ✅ Utilisation de F\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n    optimizer.step()\n    return loss.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Évaluation de l'agent\ndef evaluate_agent(policy_net, env, n_episodes=3):\n    total_rewards = []\n    steps_per_episode = []\n    for _ in range(n_episodes):\n        obs, _ = env.reset()\n        state = preprocess_observation(obs)\n        done = False\n        total_reward = 0\n        steps = 0\n        while not done and steps < MAX_STEPS_PER_EPISODE:\n            action, _ = select_action(state, policy_net, DEVICE, 0, env.action_space.n, eval_mode=True)\n            obs, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            state = preprocess_observation(obs)\n            total_reward += reward\n            steps += 1\n        total_rewards.append(total_reward)\n        steps_per_episode.append(steps)\n    return np.mean(total_rewards), np.mean(steps_per_episode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enregistrement de la vidéo\ndef record_video(model, env, video_path=\"videos/minigrid_video.mp4\", max_episodes=3):\n    frames = []\n    for ep in range(max_episodes):\n        obs, _ = env.reset()\n        state = preprocess_observation(obs)\n        done = False\n        while not done:\n            frame = env.render()  # ✅ Mode 'rgb_array' forcé à la création de l'environnement\n            if frame.ndim == 3:\n                frames.append(frame)\n            action = select_action(state, model, DEVICE, 0, env.action_space.n, eval_mode=True)[0]\n            obs, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            state = preprocess_observation(obs)\n    if len(frames) > 0:\n        imageio.mimsave(video_path, frames, fps=10)\n        print(f\"Vidéo sauvegardée : {video_path}\")\n    else:\n        print(\"Aucune frame capturée pour la vidéo.\")\n    env.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entraînement principal\ndef train_agent(num_episodes=500):\n    # Création de l'environnement avec mode 'rgb_array'\n    env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='rgb_array')\n    env = RGBImgObsWrapper(env)\n    eval_env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='rgb_array')\n    eval_env = RGBImgObsWrapper(eval_env)\n    \n    obs_shape = env.observation_space['image'].shape\n    obs_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # Format CHW\n    n_actions = env.action_space.n\n    \n    policy_net = EnhancedDQN(obs_shape, n_actions).to(DEVICE)\n    target_net = EnhancedDQN(obs_shape, n_actions).to(DEVICE)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n    \n    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n    buffer = ReplayBuffer(MEMORY_SIZE)\n    best_reward = -float('inf')\n    steps_total = 0\n    os.makedirs(\"videos\", exist_ok=True)\n    \n    for episode in range(1, num_episodes + 1):\n        obs, _ = env.reset()\n        state = preprocess_observation(obs)\n        total_reward = 0\n        done = False\n        episode_steps = 0\n        episode_loss = 0\n        while not done and episode_steps < MAX_STEPS_PER_EPISODE:\n            action, eps = select_action(state, policy_net, DEVICE, steps_total, n_actions)\n            steps_total += 1\n            episode_steps += 1\n            obs, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            next_state = preprocess_observation(obs) if not done else None\n            shaped_reward = reward * 10.0\n            if not done:\n                shaped_reward -= 0.01\n            buffer.push(state, action, next_state, shaped_reward, done)\n            state = next_state\n            total_reward += reward\n            loss = optimize(policy_net, target_net, buffer, optimizer, DEVICE)\n            if loss:\n                episode_loss += loss\n            for target_param, source_param in zip(target_net.parameters(), policy_net.parameters()):\n                target_param.data.copy_(TAU * source_param.data + (1 - TAU) * target_param.data)\n        avg_loss = episode_loss / episode_steps if episode_steps > 0 else 0\n        if episode % 10 == 0:\n            eval_reward, eval_steps = evaluate_agent(policy_net, eval_env)\n            print(f\"Évaluation Ep {episode}: Reward: {eval_reward:.2f}, Steps: {eval_steps:.1f}\")\n            if eval_reward > best_reward:\n                best_reward = eval_reward\n                torch.save({\n                    'episode': episode,\n                    'model_state_dict': policy_net.state_dict(),\n                    'reward': best_reward,\n                }, \"dqn_best_model.pth\")\n                print(f\"Meilleur modèle sauvegardé avec récompense {best_reward:.2f}\")\n        print(f\"Ep {episode:4d} | Reward: {total_reward:6.2f} | Steps: {episode_steps:3d} | Eps: {eps:.3f} | Loss: {avg_loss:.4f}\")\n    record_video(policy_net, env)\n    env.close()\n    eval_env.close()\n    return policy_net","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparamètres\nBATCH_SIZE = 64\nGAMMA = 0.99\nEPS_START = 1.0\nEPS_END = 0.01\nEPS_DECAY = 10000\nTARGET_UPDATE = 50\nLEARNING_RATE = 5e-4\nMEMORY_SIZE = 50000\nMAX_STEPS_PER_EPISODE = 500\nTAU = 0.005\nUSE_CUDA = torch.cuda.is_available()\nDEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"Démarrage de l'entraînement DQN optimisé...\")\n    trained_model = train_agent(num_episodes=500)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}