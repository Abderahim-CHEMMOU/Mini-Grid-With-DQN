{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e390cba7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T23:29:53.932574Z",
     "iopub.status.busy": "2025-05-10T23:29:53.932308Z",
     "iopub.status.idle": "2025-05-10T23:29:58.494998Z",
     "shell.execute_reply": "2025-05-10T23:29:58.494210Z"
    },
    "papermill": {
     "duration": 4.568466,
     "end_time": "2025-05-10T23:29:58.496318",
     "exception": false,
     "start_time": "2025-05-10T23:29:53.927852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (0.29.0)\r\n",
      "Collecting minigrid\r\n",
      "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.1)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from minigrid) (2.6.1)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: minigrid\r\n",
      "Successfully installed minigrid-3.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium minigrid imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9365758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:29:58.504260Z",
     "iopub.status.busy": "2025-05-10T23:29:58.503955Z",
     "iopub.status.idle": "2025-05-10T23:30:05.002966Z",
     "shell.execute_reply": "2025-05-10T23:30:05.002138Z"
    },
    "papermill": {
     "duration": 6.504845,
     "end_time": "2025-05-10T23:30:05.004908",
     "exception": false,
     "start_time": "2025-05-10T23:29:58.500063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n",
      "    _register_atari_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n",
      "    import ale_py\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/__init__.py\", line 68, in <module>\n",
      "    register_v0_v4_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n",
      "    _register_rom_configs(legacy_games, obs_types, versions)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n",
      "    gymnasium.register(\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # ✅ Pour F.smooth_l1_loss\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import gymnasium as gym\n",
    "from minigrid.envs.empty import EmptyEnv\n",
    "from minigrid.wrappers import RGBImgObsWrapper\n",
    "import imageio\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00486ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.015792Z",
     "iopub.status.busy": "2025-05-10T23:30:05.015068Z",
     "iopub.status.idle": "2025-05-10T23:30:05.019355Z",
     "shell.execute_reply": "2025-05-10T23:30:05.018670Z"
    },
    "papermill": {
     "duration": 0.010381,
     "end_time": "2025-05-10T23:30:05.020404",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.010023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autoriser les types numpy pour charger le modèle\n",
    "torch.serialization.add_safe_globals([np.core.multiarray.scalar, np.dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867ffbe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.028283Z",
     "iopub.status.busy": "2025-05-10T23:30:05.028032Z",
     "iopub.status.idle": "2025-05-10T23:30:05.036548Z",
     "shell.execute_reply": "2025-05-10T23:30:05.034792Z"
    },
    "papermill": {
     "duration": 0.014542,
     "end_time": "2025-05-10T23:30:05.037881",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.023339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Architecture du réseau\n",
    "class EnhancedDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            conv_out = self.conv(dummy).shape[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd57dba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.044749Z",
     "iopub.status.busy": "2025-05-10T23:30:05.044477Z",
     "iopub.status.idle": "2025-05-10T23:30:05.049381Z",
     "shell.execute_reply": "2025-05-10T23:30:05.048769Z"
    },
    "papermill": {
     "duration": 0.009531,
     "end_time": "2025-05-10T23:30:05.050500",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.040969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environnement personnalisé avec positions aléatoires\n",
    "class RandomGoalEmptyEnv(EmptyEnv):\n",
    "    def __init__(self, size=8):\n",
    "        super().__init__(size=size)\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._gen_grid(self.width, self.height)\n",
    "        self.agent_pos = self._empty_cell()\n",
    "        self._goal_pos = self._empty_cell()\n",
    "        return self.gen_obs(), {}\n",
    "    def _empty_cell(self):\n",
    "        while True:\n",
    "            pos = self.np_random.integers(0, self.width, size=2)\n",
    "            if self.grid.get(*pos) is None and tuple(pos) != tuple(self.agent_pos):\n",
    "                return tuple(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343da3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.059422Z",
     "iopub.status.busy": "2025-05-10T23:30:05.058844Z",
     "iopub.status.idle": "2025-05-10T23:30:05.064061Z",
     "shell.execute_reply": "2025-05-10T23:30:05.063303Z"
    },
    "papermill": {
     "duration": 0.010463,
     "end_time": "2025-05-10T23:30:05.065204",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.054741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prétraitement des observations\n",
    "def preprocess_observation(obs):\n",
    "    state = obs['image'].transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "    return (state - 0.5) / 0.5  # Normalisation à [-1, 1]\n",
    "\n",
    "# Structure de données pour le buffer de replay\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "class ReplayBuffer(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super().__init__([], maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(list(self), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36779f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.072441Z",
     "iopub.status.busy": "2025-05-10T23:30:05.072245Z",
     "iopub.status.idle": "2025-05-10T23:30:05.077064Z",
     "shell.execute_reply": "2025-05-10T23:30:05.076214Z"
    },
    "papermill": {
     "duration": 0.009363,
     "end_time": "2025-05-10T23:30:05.078178",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.068815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sélection d'action avec epsilon-greedy\n",
    "def select_action(state, policy_net, device, steps_done, n_actions, eval_mode=False):\n",
    "    eps = EPS_END + (EPS_START - EPS_END) * np.exp(-steps_done / EPS_DECAY)\n",
    "    if eval_mode or random.random() > eps:\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return policy_net(state_t).argmax().item(), eps\n",
    "    else:\n",
    "        return random.randint(0, n_actions-1), eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cc3ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.085975Z",
     "iopub.status.busy": "2025-05-10T23:30:05.085329Z",
     "iopub.status.idle": "2025-05-10T23:30:05.092897Z",
     "shell.execute_reply": "2025-05-10T23:30:05.092195Z"
    },
    "papermill": {
     "duration": 0.013033,
     "end_time": "2025-05-10T23:30:05.094078",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.081045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize(policy_net, target_net, buffer, optimizer, device):\n",
    "    if len(buffer) < BATCH_SIZE:\n",
    "        return 0.0\n",
    "    transitions = buffer.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    states = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "    actions = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "    rewards = torch.FloatTensor(batch.reward).to(device)\n",
    "    dones = torch.BoolTensor(batch.done).to(device)\n",
    "    non_final_mask = ~dones\n",
    "    non_final_next_states = torch.FloatTensor(\n",
    "        np.array([s for s, d in zip(batch.next_state, batch.done) if not d])\n",
    "    ).to(device)\n",
    "    current_q = policy_net(states).gather(1, actions)\n",
    "    next_q_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if len(non_final_next_states) > 0:\n",
    "        with torch.no_grad():\n",
    "            next_actions = policy_net(non_final_next_states).argmax(1, keepdim=True)\n",
    "            next_q_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_actions).squeeze()\n",
    "    target_values = rewards + GAMMA * next_q_values * (~dones).float()\n",
    "    loss = F.smooth_l1_loss(current_q, target_values.unsqueeze(1))  # ✅ Utilisation de F\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81f7be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.100317Z",
     "iopub.status.busy": "2025-05-10T23:30:05.100126Z",
     "iopub.status.idle": "2025-05-10T23:30:05.105123Z",
     "shell.execute_reply": "2025-05-10T23:30:05.104389Z"
    },
    "papermill": {
     "duration": 0.009639,
     "end_time": "2025-05-10T23:30:05.106603",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.096964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Évaluation de l'agent\n",
    "def evaluate_agent(policy_net, env, n_episodes=3):\n",
    "    total_rewards = []\n",
    "    steps_per_episode = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while not done and steps < MAX_STEPS_PER_EPISODE:\n",
    "            action, _ = select_action(state, policy_net, DEVICE, 0, env.action_space.n, eval_mode=True)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = preprocess_observation(obs)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        total_rewards.append(total_reward)\n",
    "        steps_per_episode.append(steps)\n",
    "    return np.mean(total_rewards), np.mean(steps_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5092df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.117112Z",
     "iopub.status.busy": "2025-05-10T23:30:05.116668Z",
     "iopub.status.idle": "2025-05-10T23:30:05.121858Z",
     "shell.execute_reply": "2025-05-10T23:30:05.121197Z"
    },
    "papermill": {
     "duration": 0.011808,
     "end_time": "2025-05-10T23:30:05.123176",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.111368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enregistrement de la vidéo\n",
    "def record_video(model, env, video_path=\"videos/minigrid_video.mp4\", max_episodes=3):\n",
    "    frames = []\n",
    "    for ep in range(max_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        done = False\n",
    "        while not done:\n",
    "            frame = env.render()  # ✅ Mode 'rgb_array' forcé à la création de l'environnement\n",
    "            if frame.ndim == 3:\n",
    "                frames.append(frame)\n",
    "            action = select_action(state, model, DEVICE, 0, env.action_space.n, eval_mode=True)[0]\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = preprocess_observation(obs)\n",
    "    if len(frames) > 0:\n",
    "        imageio.mimsave(video_path, frames, fps=10)\n",
    "        print(f\"Vidéo sauvegardée : {video_path}\")\n",
    "    else:\n",
    "        print(\"Aucune frame capturée pour la vidéo.\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d032a65b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.130676Z",
     "iopub.status.busy": "2025-05-10T23:30:05.130414Z",
     "iopub.status.idle": "2025-05-10T23:30:05.145425Z",
     "shell.execute_reply": "2025-05-10T23:30:05.144771Z"
    },
    "papermill": {
     "duration": 0.019499,
     "end_time": "2025-05-10T23:30:05.146399",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.126900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entraînement principal\n",
    "def train_agent(num_episodes=500):\n",
    "    # Création de l'environnement avec mode 'rgb_array'\n",
    "    env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='rgb_array')\n",
    "    env = RGBImgObsWrapper(env)\n",
    "    eval_env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='rgb_array')\n",
    "    eval_env = RGBImgObsWrapper(eval_env)\n",
    "    \n",
    "    obs_shape = env.observation_space['image'].shape\n",
    "    obs_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # Format CHW\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    policy_net = EnhancedDQN(obs_shape, n_actions).to(DEVICE)\n",
    "    target_net = EnhancedDQN(obs_shape, n_actions).to(DEVICE)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "    best_reward = -float('inf')\n",
    "    steps_total = 0\n",
    "    os.makedirs(\"videos\", exist_ok=True)\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        episode_steps = 0\n",
    "        episode_loss = 0\n",
    "        while not done and episode_steps < MAX_STEPS_PER_EPISODE:\n",
    "            action, eps = select_action(state, policy_net, DEVICE, steps_total, n_actions)\n",
    "            steps_total += 1\n",
    "            episode_steps += 1\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = preprocess_observation(obs) if not done else None\n",
    "            shaped_reward = reward * 10.0\n",
    "            if not done:\n",
    "                shaped_reward -= 0.01\n",
    "            buffer.push(state, action, next_state, shaped_reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            loss = optimize(policy_net, target_net, buffer, optimizer, DEVICE)\n",
    "            if loss:\n",
    "                episode_loss += loss\n",
    "            for target_param, source_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "                target_param.data.copy_(TAU * source_param.data + (1 - TAU) * target_param.data)\n",
    "        avg_loss = episode_loss / episode_steps if episode_steps > 0 else 0\n",
    "        if episode % 10 == 0:\n",
    "            eval_reward, eval_steps = evaluate_agent(policy_net, eval_env)\n",
    "            print(f\"Évaluation Ep {episode}: Reward: {eval_reward:.2f}, Steps: {eval_steps:.1f}\")\n",
    "            if eval_reward > best_reward:\n",
    "                best_reward = eval_reward\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': policy_net.state_dict(),\n",
    "                    'reward': best_reward,\n",
    "                }, \"dqn_best_model.pth\")\n",
    "                print(f\"Meilleur modèle sauvegardé avec récompense {best_reward:.2f}\")\n",
    "        print(f\"Ep {episode:4d} | Reward: {total_reward:6.2f} | Steps: {episode_steps:3d} | Eps: {eps:.3f} | Loss: {avg_loss:.4f}\")\n",
    "    record_video(policy_net, env)\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddab11b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.152648Z",
     "iopub.status.busy": "2025-05-10T23:30:05.152402Z",
     "iopub.status.idle": "2025-05-10T23:30:05.258116Z",
     "shell.execute_reply": "2025-05-10T23:30:05.257341Z"
    },
    "papermill": {
     "duration": 0.110198,
     "end_time": "2025-05-10T23:30:05.259358",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.149160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "MEMORY_SIZE = 50000\n",
    "MAX_STEPS_PER_EPISODE = 500\n",
    "TAU = 0.005\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149ad742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T23:30:05.266605Z",
     "iopub.status.busy": "2025-05-10T23:30:05.266307Z",
     "iopub.status.idle": "2025-05-10T23:37:23.310778Z",
     "shell.execute_reply": "2025-05-10T23:37:23.309589Z"
    },
    "papermill": {
     "duration": 438.049959,
     "end_time": "2025-05-10T23:37:23.312401",
     "exception": false,
     "start_time": "2025-05-10T23:30:05.262442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'entraînement DQN optimisé...\n",
      "Ep    1 | Reward:   0.00 | Steps: 256 | Eps: 0.975 | Loss: 0.0002\n",
      "Ep    2 | Reward:   0.00 | Steps: 256 | Eps: 0.951 | Loss: 0.0001\n",
      "Ep    3 | Reward:   0.00 | Steps: 256 | Eps: 0.927 | Loss: 0.0000\n",
      "Ep    4 | Reward:   0.00 | Steps: 256 | Eps: 0.904 | Loss: 0.0000\n",
      "Ep    5 | Reward:   0.52 | Steps: 137 | Eps: 0.892 | Loss: 0.0000\n",
      "Ep    6 | Reward:   0.72 | Steps:  81 | Eps: 0.884 | Loss: 0.0046\n",
      "Ep    7 | Reward:   0.00 | Steps: 256 | Eps: 0.862 | Loss: 0.0050\n",
      "Ep    8 | Reward:   0.45 | Steps: 157 | Eps: 0.849 | Loss: 0.0083\n",
      "Ep    9 | Reward:   0.63 | Steps: 106 | Eps: 0.840 | Loss: 0.0082\n",
      "Évaluation Ep 10: Reward: 0.00, Steps: 256.0\n",
      "Meilleur modèle sauvegardé avec récompense 0.00\n",
      "Ep   10 | Reward:   0.66 | Steps:  96 | Eps: 0.832 | Loss: 0.0066\n",
      "Ep   11 | Reward:   0.64 | Steps: 103 | Eps: 0.824 | Loss: 0.0158\n",
      "Ep   12 | Reward:   0.30 | Steps: 198 | Eps: 0.808 | Loss: 0.0128\n",
      "Ep   13 | Reward:   0.81 | Steps:  55 | Eps: 0.804 | Loss: 0.0106\n",
      "Ep   14 | Reward:   0.72 | Steps:  79 | Eps: 0.797 | Loss: 0.0107\n",
      "Ep   15 | Reward:   0.52 | Steps: 136 | Eps: 0.787 | Loss: 0.0172\n",
      "Ep   16 | Reward:   0.72 | Steps:  79 | Eps: 0.781 | Loss: 0.0145\n",
      "Ep   17 | Reward:   0.67 | Steps:  95 | Eps: 0.773 | Loss: 0.0191\n",
      "Ep   18 | Reward:   0.81 | Steps:  53 | Eps: 0.769 | Loss: 0.0124\n",
      "Ep   19 | Reward:   0.75 | Steps:  71 | Eps: 0.764 | Loss: 0.0111\n",
      "Évaluation Ep 20: Reward: 0.00, Steps: 256.0\n",
      "Ep   20 | Reward:   0.75 | Steps:  71 | Eps: 0.759 | Loss: 0.0125\n",
      "Ep   21 | Reward:   0.66 | Steps:  96 | Eps: 0.751 | Loss: 0.0082\n",
      "Ep   22 | Reward:   0.60 | Steps: 114 | Eps: 0.743 | Loss: 0.0145\n",
      "Ep   23 | Reward:   0.81 | Steps:  53 | Eps: 0.739 | Loss: 0.0143\n",
      "Ep   24 | Reward:   0.88 | Steps:  34 | Eps: 0.737 | Loss: 0.0108\n",
      "Ep   25 | Reward:   0.88 | Steps:  35 | Eps: 0.734 | Loss: 0.0191\n",
      "Ep   26 | Reward:   0.90 | Steps:  28 | Eps: 0.732 | Loss: 0.0139\n",
      "Ep   27 | Reward:   0.75 | Steps:  71 | Eps: 0.727 | Loss: 0.0135\n",
      "Ep   28 | Reward:   0.86 | Steps:  41 | Eps: 0.724 | Loss: 0.0171\n",
      "Ep   29 | Reward:   0.85 | Steps:  44 | Eps: 0.721 | Loss: 0.0123\n",
      "Évaluation Ep 30: Reward: 0.96, Steps: 12.0\n",
      "Meilleur modèle sauvegardé avec récompense 0.96\n",
      "Ep   30 | Reward:   0.73 | Steps:  77 | Eps: 0.715 | Loss: 0.0139\n",
      "Ep   31 | Reward:   0.92 | Steps:  22 | Eps: 0.714 | Loss: 0.0183\n",
      "Ep   32 | Reward:   0.85 | Steps:  42 | Eps: 0.711 | Loss: 0.0154\n",
      "Ep   33 | Reward:   0.76 | Steps:  67 | Eps: 0.706 | Loss: 0.0190\n",
      "Ep   34 | Reward:   0.76 | Steps:  67 | Eps: 0.702 | Loss: 0.0122\n",
      "Ep   35 | Reward:   0.79 | Steps:  59 | Eps: 0.698 | Loss: 0.0189\n",
      "Ep   36 | Reward:   0.78 | Steps:  63 | Eps: 0.693 | Loss: 0.0181\n",
      "Ep   37 | Reward:   0.90 | Steps:  29 | Eps: 0.691 | Loss: 0.0131\n",
      "Ep   38 | Reward:   0.82 | Steps:  51 | Eps: 0.688 | Loss: 0.0165\n",
      "Ep   39 | Reward:   0.84 | Steps:  46 | Eps: 0.685 | Loss: 0.0156\n",
      "Évaluation Ep 40: Reward: 0.96, Steps: 12.0\n",
      "Ep   40 | Reward:   0.84 | Steps:  45 | Eps: 0.682 | Loss: 0.0175\n",
      "Ep   41 | Reward:   0.86 | Steps:  40 | Eps: 0.679 | Loss: 0.0219\n",
      "Ep   42 | Reward:   0.85 | Steps:  43 | Eps: 0.676 | Loss: 0.0136\n",
      "Ep   43 | Reward:   0.88 | Steps:  35 | Eps: 0.674 | Loss: 0.0236\n",
      "Ep   44 | Reward:   0.92 | Steps:  24 | Eps: 0.672 | Loss: 0.0200\n",
      "Ep   45 | Reward:   0.90 | Steps:  29 | Eps: 0.670 | Loss: 0.0139\n",
      "Ep   46 | Reward:   0.86 | Steps:  39 | Eps: 0.668 | Loss: 0.0112\n",
      "Ep   47 | Reward:   0.82 | Steps:  50 | Eps: 0.664 | Loss: 0.0118\n",
      "Ep   48 | Reward:   0.90 | Steps:  28 | Eps: 0.663 | Loss: 0.0158\n",
      "Ep   49 | Reward:   0.92 | Steps:  24 | Eps: 0.661 | Loss: 0.0130\n",
      "Évaluation Ep 50: Reward: 0.96, Steps: 12.0\n",
      "Ep   50 | Reward:   0.81 | Steps:  54 | Eps: 0.657 | Loss: 0.0176\n",
      "Ep   51 | Reward:   0.78 | Steps:  64 | Eps: 0.653 | Loss: 0.0142\n",
      "Ep   52 | Reward:   0.90 | Steps:  29 | Eps: 0.651 | Loss: 0.0192\n",
      "Ep   53 | Reward:   0.85 | Steps:  43 | Eps: 0.649 | Loss: 0.0148\n",
      "Ep   54 | Reward:   0.82 | Steps:  51 | Eps: 0.645 | Loss: 0.0166\n",
      "Ep   55 | Reward:   0.86 | Steps:  40 | Eps: 0.643 | Loss: 0.0169\n",
      "Ep   56 | Reward:   0.87 | Steps:  36 | Eps: 0.641 | Loss: 0.0135\n",
      "Ep   57 | Reward:   0.89 | Steps:  30 | Eps: 0.639 | Loss: 0.0094\n",
      "Ep   58 | Reward:   0.85 | Steps:  43 | Eps: 0.636 | Loss: 0.0189\n",
      "Ep   59 | Reward:   0.86 | Steps:  41 | Eps: 0.634 | Loss: 0.0101\n",
      "Évaluation Ep 60: Reward: 0.96, Steps: 12.0\n",
      "Ep   60 | Reward:   0.86 | Steps:  41 | Eps: 0.631 | Loss: 0.0232\n",
      "Ep   61 | Reward:   0.90 | Steps:  29 | Eps: 0.629 | Loss: 0.0153\n",
      "Ep   62 | Reward:   0.89 | Steps:  30 | Eps: 0.627 | Loss: 0.0088\n",
      "Ep   63 | Reward:   0.93 | Steps:  21 | Eps: 0.626 | Loss: 0.0089\n",
      "Ep   64 | Reward:   0.87 | Steps:  38 | Eps: 0.624 | Loss: 0.0154\n",
      "Ep   65 | Reward:   0.89 | Steps:  32 | Eps: 0.622 | Loss: 0.0087\n",
      "Ep   66 | Reward:   0.81 | Steps:  55 | Eps: 0.618 | Loss: 0.0100\n",
      "Ep   67 | Reward:   0.89 | Steps:  31 | Eps: 0.617 | Loss: 0.0252\n",
      "Ep   68 | Reward:   0.88 | Steps:  33 | Eps: 0.615 | Loss: 0.0173\n",
      "Ep   69 | Reward:   0.71 | Steps:  82 | Eps: 0.610 | Loss: 0.0130\n",
      "Évaluation Ep 70: Reward: 0.96, Steps: 12.0\n",
      "Ep   70 | Reward:   0.90 | Steps:  29 | Eps: 0.608 | Loss: 0.0152\n",
      "Ep   71 | Reward:   0.92 | Steps:  24 | Eps: 0.606 | Loss: 0.0130\n",
      "Ep   72 | Reward:   0.91 | Steps:  25 | Eps: 0.605 | Loss: 0.0122\n",
      "Ep   73 | Reward:   0.91 | Steps:  27 | Eps: 0.603 | Loss: 0.0110\n",
      "Ep   74 | Reward:   0.77 | Steps:  66 | Eps: 0.599 | Loss: 0.0074\n",
      "Ep   75 | Reward:   0.72 | Steps:  81 | Eps: 0.595 | Loss: 0.0183\n",
      "Ep   76 | Reward:   0.81 | Steps:  53 | Eps: 0.592 | Loss: 0.0080\n",
      "Ep   77 | Reward:   0.88 | Steps:  34 | Eps: 0.590 | Loss: 0.0167\n",
      "Ep   78 | Reward:   0.87 | Steps:  36 | Eps: 0.587 | Loss: 0.0176\n",
      "Ep   79 | Reward:   0.78 | Steps:  64 | Eps: 0.584 | Loss: 0.0071\n",
      "Évaluation Ep 80: Reward: 0.96, Steps: 12.0\n",
      "Ep   80 | Reward:   0.94 | Steps:  18 | Eps: 0.583 | Loss: 0.0050\n",
      "Ep   81 | Reward:   0.89 | Steps:  31 | Eps: 0.581 | Loss: 0.0172\n",
      "Ep   82 | Reward:   0.91 | Steps:  25 | Eps: 0.580 | Loss: 0.0160\n",
      "Ep   83 | Reward:   0.84 | Steps:  46 | Eps: 0.577 | Loss: 0.0187\n",
      "Ep   84 | Reward:   0.91 | Steps:  26 | Eps: 0.575 | Loss: 0.0153\n",
      "Ep   85 | Reward:   0.92 | Steps:  22 | Eps: 0.574 | Loss: 0.0115\n",
      "Ep   86 | Reward:   0.92 | Steps:  22 | Eps: 0.573 | Loss: 0.0108\n",
      "Ep   87 | Reward:   0.86 | Steps:  39 | Eps: 0.571 | Loss: 0.0114\n",
      "Ep   88 | Reward:   0.92 | Steps:  22 | Eps: 0.570 | Loss: 0.0159\n",
      "Ep   89 | Reward:   0.89 | Steps:  30 | Eps: 0.568 | Loss: 0.0153\n",
      "Évaluation Ep 90: Reward: 0.96, Steps: 12.0\n",
      "Ep   90 | Reward:   0.90 | Steps:  28 | Eps: 0.566 | Loss: 0.0171\n",
      "Ep   91 | Reward:   0.88 | Steps:  33 | Eps: 0.565 | Loss: 0.0082\n",
      "Ep   92 | Reward:   0.89 | Steps:  30 | Eps: 0.563 | Loss: 0.0080\n",
      "Ep   93 | Reward:   0.92 | Steps:  22 | Eps: 0.562 | Loss: 0.0096\n",
      "Ep   94 | Reward:   0.93 | Steps:  19 | Eps: 0.561 | Loss: 0.0085\n",
      "Ep   95 | Reward:   0.93 | Steps:  21 | Eps: 0.559 | Loss: 0.0156\n",
      "Ep   96 | Reward:   0.91 | Steps:  25 | Eps: 0.558 | Loss: 0.0080\n",
      "Ep   97 | Reward:   0.93 | Steps:  19 | Eps: 0.557 | Loss: 0.0148\n",
      "Ep   98 | Reward:   0.93 | Steps:  20 | Eps: 0.556 | Loss: 0.0059\n",
      "Ep   99 | Reward:   0.91 | Steps:  26 | Eps: 0.555 | Loss: 0.0079\n",
      "Évaluation Ep 100: Reward: 0.96, Steps: 12.0\n",
      "Ep  100 | Reward:   0.95 | Steps:  15 | Eps: 0.554 | Loss: 0.0128\n",
      "Ep  101 | Reward:   0.90 | Steps:  28 | Eps: 0.552 | Loss: 0.0145\n",
      "Ep  102 | Reward:   0.94 | Steps:  18 | Eps: 0.551 | Loss: 0.0163\n",
      "Ep  103 | Reward:   0.90 | Steps:  28 | Eps: 0.550 | Loss: 0.0118\n",
      "Ep  104 | Reward:   0.91 | Steps:  26 | Eps: 0.548 | Loss: 0.0223\n",
      "Ep  105 | Reward:   0.93 | Steps:  19 | Eps: 0.547 | Loss: 0.0189\n",
      "Ep  106 | Reward:   0.90 | Steps:  29 | Eps: 0.546 | Loss: 0.0089\n",
      "Ep  107 | Reward:   0.93 | Steps:  19 | Eps: 0.545 | Loss: 0.0158\n",
      "Ep  108 | Reward:   0.87 | Steps:  36 | Eps: 0.543 | Loss: 0.0130\n",
      "Ep  109 | Reward:   0.94 | Steps:  17 | Eps: 0.542 | Loss: 0.0140\n",
      "Évaluation Ep 110: Reward: 0.96, Steps: 12.0\n",
      "Ep  110 | Reward:   0.84 | Steps:  45 | Eps: 0.539 | Loss: 0.0147\n",
      "Ep  111 | Reward:   0.90 | Steps:  29 | Eps: 0.538 | Loss: 0.0142\n",
      "Ep  112 | Reward:   0.91 | Steps:  27 | Eps: 0.537 | Loss: 0.0094\n",
      "Ep  113 | Reward:   0.88 | Steps:  33 | Eps: 0.535 | Loss: 0.0129\n",
      "Ep  114 | Reward:   0.79 | Steps:  59 | Eps: 0.532 | Loss: 0.0161\n",
      "Ep  115 | Reward:   0.92 | Steps:  23 | Eps: 0.531 | Loss: 0.0210\n",
      "Ep  116 | Reward:   0.57 | Steps: 123 | Eps: 0.524 | Loss: 0.0135\n",
      "Ep  117 | Reward:   0.92 | Steps:  23 | Eps: 0.523 | Loss: 0.0222\n",
      "Ep  118 | Reward:   0.92 | Steps:  23 | Eps: 0.522 | Loss: 0.0084\n",
      "Ep  119 | Reward:   0.86 | Steps:  39 | Eps: 0.520 | Loss: 0.0082\n",
      "Évaluation Ep 120: Reward: 0.96, Steps: 12.0\n",
      "Ep  120 | Reward:   0.88 | Steps:  33 | Eps: 0.518 | Loss: 0.0119\n",
      "Ep  121 | Reward:   0.89 | Steps:  31 | Eps: 0.517 | Loss: 0.0147\n",
      "Ep  122 | Reward:   0.91 | Steps:  25 | Eps: 0.515 | Loss: 0.0114\n",
      "Ep  123 | Reward:   0.94 | Steps:  16 | Eps: 0.514 | Loss: 0.0079\n",
      "Ep  124 | Reward:   0.92 | Steps:  24 | Eps: 0.513 | Loss: 0.0165\n",
      "Ep  125 | Reward:   0.88 | Steps:  33 | Eps: 0.512 | Loss: 0.0114\n",
      "Ep  126 | Reward:   0.86 | Steps:  40 | Eps: 0.510 | Loss: 0.0195\n",
      "Ep  127 | Reward:   0.91 | Steps:  25 | Eps: 0.508 | Loss: 0.0161\n",
      "Ep  128 | Reward:   0.90 | Steps:  28 | Eps: 0.507 | Loss: 0.0167\n",
      "Ep  129 | Reward:   0.89 | Steps:  32 | Eps: 0.505 | Loss: 0.0085\n",
      "Évaluation Ep 130: Reward: 0.96, Steps: 12.0\n",
      "Ep  130 | Reward:   0.88 | Steps:  34 | Eps: 0.504 | Loss: 0.0094\n",
      "Ep  131 | Reward:   0.92 | Steps:  23 | Eps: 0.503 | Loss: 0.0177\n",
      "Ep  132 | Reward:   0.89 | Steps:  31 | Eps: 0.501 | Loss: 0.0101\n",
      "Ep  133 | Reward:   0.91 | Steps:  26 | Eps: 0.500 | Loss: 0.0104\n",
      "Ep  134 | Reward:   0.91 | Steps:  26 | Eps: 0.498 | Loss: 0.0171\n",
      "Ep  135 | Reward:   0.85 | Steps:  42 | Eps: 0.496 | Loss: 0.0190\n",
      "Ep  136 | Reward:   0.92 | Steps:  22 | Eps: 0.495 | Loss: 0.0126\n",
      "Ep  137 | Reward:   0.86 | Steps:  41 | Eps: 0.493 | Loss: 0.0119\n",
      "Ep  138 | Reward:   0.87 | Steps:  38 | Eps: 0.492 | Loss: 0.0064\n",
      "Ep  139 | Reward:   0.89 | Steps:  30 | Eps: 0.490 | Loss: 0.0203\n",
      "Évaluation Ep 140: Reward: 0.96, Steps: 12.0\n",
      "Ep  140 | Reward:   0.91 | Steps:  25 | Eps: 0.489 | Loss: 0.0065\n",
      "Ep  141 | Reward:   0.92 | Steps:  24 | Eps: 0.488 | Loss: 0.0066\n",
      "Ep  142 | Reward:   0.91 | Steps:  25 | Eps: 0.487 | Loss: 0.0214\n",
      "Ep  143 | Reward:   0.91 | Steps:  25 | Eps: 0.485 | Loss: 0.0053\n",
      "Ep  144 | Reward:   0.87 | Steps:  37 | Eps: 0.484 | Loss: 0.0097\n",
      "Ep  145 | Reward:   0.90 | Steps:  29 | Eps: 0.482 | Loss: 0.0078\n",
      "Ep  146 | Reward:   0.95 | Steps:  15 | Eps: 0.482 | Loss: 0.0101\n",
      "Ep  147 | Reward:   0.92 | Steps:  23 | Eps: 0.480 | Loss: 0.0110\n",
      "Ep  148 | Reward:   0.89 | Steps:  32 | Eps: 0.479 | Loss: 0.0134\n",
      "Ep  149 | Reward:   0.93 | Steps:  21 | Eps: 0.478 | Loss: 0.0111\n",
      "Évaluation Ep 150: Reward: 0.96, Steps: 12.0\n",
      "Ep  150 | Reward:   0.88 | Steps:  34 | Eps: 0.476 | Loss: 0.0117\n",
      "Ep  151 | Reward:   0.92 | Steps:  22 | Eps: 0.475 | Loss: 0.0079\n",
      "Ep  152 | Reward:   0.94 | Steps:  18 | Eps: 0.475 | Loss: 0.0098\n",
      "Ep  153 | Reward:   0.92 | Steps:  24 | Eps: 0.473 | Loss: 0.0199\n",
      "Ep  154 | Reward:   0.94 | Steps:  18 | Eps: 0.473 | Loss: 0.0123\n",
      "Ep  155 | Reward:   0.87 | Steps:  38 | Eps: 0.471 | Loss: 0.0118\n",
      "Ep  156 | Reward:   0.92 | Steps:  23 | Eps: 0.470 | Loss: 0.0094\n",
      "Ep  157 | Reward:   0.92 | Steps:  24 | Eps: 0.469 | Loss: 0.0142\n",
      "Ep  158 | Reward:   0.92 | Steps:  24 | Eps: 0.468 | Loss: 0.0042\n",
      "Ep  159 | Reward:   0.91 | Steps:  25 | Eps: 0.466 | Loss: 0.0120\n",
      "Évaluation Ep 160: Reward: 0.00, Steps: 256.0\n",
      "Ep  160 | Reward:   0.89 | Steps:  30 | Eps: 0.465 | Loss: 0.0114\n",
      "Ep  161 | Reward:   0.95 | Steps:  14 | Eps: 0.464 | Loss: 0.0183\n",
      "Ep  162 | Reward:   0.94 | Steps:  18 | Eps: 0.464 | Loss: 0.0058\n",
      "Ep  163 | Reward:   0.90 | Steps:  29 | Eps: 0.462 | Loss: 0.0081\n",
      "Ep  164 | Reward:   0.95 | Steps:  13 | Eps: 0.462 | Loss: 0.0103\n",
      "Ep  165 | Reward:   0.89 | Steps:  30 | Eps: 0.460 | Loss: 0.0032\n",
      "Ep  166 | Reward:   0.93 | Steps:  20 | Eps: 0.459 | Loss: 0.0106\n",
      "Ep  167 | Reward:   0.90 | Steps:  29 | Eps: 0.458 | Loss: 0.0084\n",
      "Ep  168 | Reward:   0.94 | Steps:  16 | Eps: 0.457 | Loss: 0.0062\n",
      "Ep  169 | Reward:   0.95 | Steps:  15 | Eps: 0.457 | Loss: 0.0187\n",
      "Évaluation Ep 170: Reward: 0.96, Steps: 12.0\n",
      "Ep  170 | Reward:   0.94 | Steps:  17 | Eps: 0.456 | Loss: 0.0141\n",
      "Ep  171 | Reward:   0.95 | Steps:  15 | Eps: 0.455 | Loss: 0.0066\n",
      "Ep  172 | Reward:   0.92 | Steps:  24 | Eps: 0.454 | Loss: 0.0164\n",
      "Ep  173 | Reward:   0.95 | Steps:  13 | Eps: 0.454 | Loss: 0.0173\n",
      "Ep  174 | Reward:   0.94 | Steps:  18 | Eps: 0.453 | Loss: 0.0162\n",
      "Ep  175 | Reward:   0.94 | Steps:  16 | Eps: 0.452 | Loss: 0.0032\n",
      "Ep  176 | Reward:   0.95 | Steps:  15 | Eps: 0.452 | Loss: 0.0167\n",
      "Ep  177 | Reward:   0.91 | Steps:  27 | Eps: 0.450 | Loss: 0.0187\n",
      "Ep  178 | Reward:   0.93 | Steps:  21 | Eps: 0.449 | Loss: 0.0089\n",
      "Ep  179 | Reward:   0.92 | Steps:  22 | Eps: 0.448 | Loss: 0.0156\n",
      "Évaluation Ep 180: Reward: 0.96, Steps: 12.0\n",
      "Ep  180 | Reward:   0.93 | Steps:  21 | Eps: 0.448 | Loss: 0.0053\n",
      "Ep  181 | Reward:   0.90 | Steps:  28 | Eps: 0.446 | Loss: 0.0087\n",
      "Ep  182 | Reward:   0.93 | Steps:  19 | Eps: 0.445 | Loss: 0.0176\n",
      "Ep  183 | Reward:   0.92 | Steps:  22 | Eps: 0.445 | Loss: 0.0050\n",
      "Ep  184 | Reward:   0.94 | Steps:  17 | Eps: 0.444 | Loss: 0.0066\n",
      "Ep  185 | Reward:   0.89 | Steps:  32 | Eps: 0.442 | Loss: 0.0101\n",
      "Ep  186 | Reward:   0.94 | Steps:  17 | Eps: 0.442 | Loss: 0.0202\n",
      "Ep  187 | Reward:   0.94 | Steps:  18 | Eps: 0.441 | Loss: 0.0180\n",
      "Ep  188 | Reward:   0.92 | Steps:  23 | Eps: 0.440 | Loss: 0.0081\n",
      "Ep  189 | Reward:   0.93 | Steps:  21 | Eps: 0.439 | Loss: 0.0048\n",
      "Évaluation Ep 190: Reward: 0.96, Steps: 12.0\n",
      "Ep  190 | Reward:   0.94 | Steps:  18 | Eps: 0.438 | Loss: 0.0110\n",
      "Ep  191 | Reward:   0.88 | Steps:  34 | Eps: 0.437 | Loss: 0.0049\n",
      "Ep  192 | Reward:   0.94 | Steps:  18 | Eps: 0.436 | Loss: 0.0087\n",
      "Ep  193 | Reward:   0.91 | Steps:  25 | Eps: 0.435 | Loss: 0.0071\n",
      "Ep  194 | Reward:   0.90 | Steps:  28 | Eps: 0.434 | Loss: 0.0154\n",
      "Ep  195 | Reward:   0.94 | Steps:  18 | Eps: 0.433 | Loss: 0.0091\n",
      "Ep  196 | Reward:   0.90 | Steps:  28 | Eps: 0.432 | Loss: 0.0101\n",
      "Ep  197 | Reward:   0.90 | Steps:  29 | Eps: 0.431 | Loss: 0.0079\n",
      "Ep  198 | Reward:   0.93 | Steps:  19 | Eps: 0.430 | Loss: 0.0102\n",
      "Ep  199 | Reward:   0.91 | Steps:  26 | Eps: 0.429 | Loss: 0.0053\n",
      "Évaluation Ep 200: Reward: 0.96, Steps: 12.0\n",
      "Ep  200 | Reward:   0.93 | Steps:  19 | Eps: 0.428 | Loss: 0.0170\n",
      "Ep  201 | Reward:   0.92 | Steps:  24 | Eps: 0.427 | Loss: 0.0172\n",
      "Ep  202 | Reward:   0.93 | Steps:  21 | Eps: 0.426 | Loss: 0.0154\n",
      "Ep  203 | Reward:   0.93 | Steps:  21 | Eps: 0.425 | Loss: 0.0106\n",
      "Ep  204 | Reward:   0.84 | Steps:  45 | Eps: 0.423 | Loss: 0.0106\n",
      "Ep  205 | Reward:   0.93 | Steps:  21 | Eps: 0.422 | Loss: 0.0137\n",
      "Ep  206 | Reward:   0.93 | Steps:  20 | Eps: 0.422 | Loss: 0.0060\n",
      "Ep  207 | Reward:   0.95 | Steps:  13 | Eps: 0.421 | Loss: 0.0060\n",
      "Ep  208 | Reward:   0.92 | Steps:  24 | Eps: 0.420 | Loss: 0.0121\n",
      "Ep  209 | Reward:   0.93 | Steps:  20 | Eps: 0.419 | Loss: 0.0048\n",
      "Évaluation Ep 210: Reward: 0.96, Steps: 12.0\n",
      "Ep  210 | Reward:   0.94 | Steps:  16 | Eps: 0.419 | Loss: 0.0101\n",
      "Ep  211 | Reward:   0.93 | Steps:  21 | Eps: 0.418 | Loss: 0.0121\n",
      "Ep  212 | Reward:   0.93 | Steps:  19 | Eps: 0.417 | Loss: 0.0051\n",
      "Ep  213 | Reward:   0.89 | Steps:  31 | Eps: 0.416 | Loss: 0.0071\n",
      "Ep  214 | Reward:   0.93 | Steps:  19 | Eps: 0.415 | Loss: 0.0115\n",
      "Ep  215 | Reward:   0.92 | Steps:  24 | Eps: 0.414 | Loss: 0.0067\n",
      "Ep  216 | Reward:   0.94 | Steps:  16 | Eps: 0.413 | Loss: 0.0142\n",
      "Ep  217 | Reward:   0.92 | Steps:  23 | Eps: 0.412 | Loss: 0.0063\n",
      "Ep  218 | Reward:   0.93 | Steps:  19 | Eps: 0.412 | Loss: 0.0101\n",
      "Ep  219 | Reward:   0.90 | Steps:  28 | Eps: 0.410 | Loss: 0.0121\n",
      "Évaluation Ep 220: Reward: 0.96, Steps: 12.0\n",
      "Ep  220 | Reward:   0.95 | Steps:  14 | Eps: 0.410 | Loss: 0.0047\n",
      "Ep  221 | Reward:   0.92 | Steps:  23 | Eps: 0.409 | Loss: 0.0044\n",
      "Ep  222 | Reward:   0.93 | Steps:  21 | Eps: 0.408 | Loss: 0.0061\n",
      "Ep  223 | Reward:   0.92 | Steps:  23 | Eps: 0.407 | Loss: 0.0059\n",
      "Ep  224 | Reward:   0.89 | Steps:  32 | Eps: 0.406 | Loss: 0.0135\n",
      "Ep  225 | Reward:   0.92 | Steps:  22 | Eps: 0.405 | Loss: 0.0158\n",
      "Ep  226 | Reward:   0.88 | Steps:  34 | Eps: 0.404 | Loss: 0.0069\n",
      "Ep  227 | Reward:   0.93 | Steps:  20 | Eps: 0.403 | Loss: 0.0139\n",
      "Ep  228 | Reward:   0.94 | Steps:  17 | Eps: 0.402 | Loss: 0.0047\n",
      "Ep  229 | Reward:   0.93 | Steps:  20 | Eps: 0.402 | Loss: 0.0117\n",
      "Évaluation Ep 230: Reward: 0.96, Steps: 12.0\n",
      "Ep  230 | Reward:   0.91 | Steps:  25 | Eps: 0.401 | Loss: 0.0056\n",
      "Ep  231 | Reward:   0.92 | Steps:  22 | Eps: 0.400 | Loss: 0.0047\n",
      "Ep  232 | Reward:   0.91 | Steps:  27 | Eps: 0.399 | Loss: 0.0070\n",
      "Ep  233 | Reward:   0.93 | Steps:  19 | Eps: 0.398 | Loss: 0.0134\n",
      "Ep  234 | Reward:   0.94 | Steps:  16 | Eps: 0.397 | Loss: 0.0127\n",
      "Ep  235 | Reward:   0.90 | Steps:  29 | Eps: 0.396 | Loss: 0.0046\n",
      "Ep  236 | Reward:   0.94 | Steps:  18 | Eps: 0.395 | Loss: 0.0102\n",
      "Ep  237 | Reward:   0.94 | Steps:  18 | Eps: 0.395 | Loss: 0.0100\n",
      "Ep  238 | Reward:   0.95 | Steps:  14 | Eps: 0.394 | Loss: 0.0112\n",
      "Ep  239 | Reward:   0.93 | Steps:  20 | Eps: 0.393 | Loss: 0.0030\n",
      "Évaluation Ep 240: Reward: 0.96, Steps: 12.0\n",
      "Ep  240 | Reward:   0.93 | Steps:  20 | Eps: 0.393 | Loss: 0.0061\n",
      "Ep  241 | Reward:   0.94 | Steps:  18 | Eps: 0.392 | Loss: 0.0034\n",
      "Ep  242 | Reward:   0.93 | Steps:  20 | Eps: 0.391 | Loss: 0.0043\n",
      "Ep  243 | Reward:   0.95 | Steps:  15 | Eps: 0.391 | Loss: 0.0039\n",
      "Ep  244 | Reward:   0.94 | Steps:  17 | Eps: 0.390 | Loss: 0.0058\n",
      "Ep  245 | Reward:   0.95 | Steps:  13 | Eps: 0.390 | Loss: 0.0140\n",
      "Ep  246 | Reward:   0.94 | Steps:  17 | Eps: 0.389 | Loss: 0.0023\n",
      "Ep  247 | Reward:   0.86 | Steps:  40 | Eps: 0.387 | Loss: 0.0133\n",
      "Ep  248 | Reward:   0.95 | Steps:  14 | Eps: 0.387 | Loss: 0.0030\n",
      "Ep  249 | Reward:   0.93 | Steps:  21 | Eps: 0.386 | Loss: 0.0028\n",
      "Évaluation Ep 250: Reward: 0.96, Steps: 12.0\n",
      "Ep  250 | Reward:   0.91 | Steps:  25 | Eps: 0.385 | Loss: 0.0120\n",
      "Ep  251 | Reward:   0.88 | Steps:  34 | Eps: 0.384 | Loss: 0.0122\n",
      "Ep  252 | Reward:   0.93 | Steps:  19 | Eps: 0.383 | Loss: 0.0171\n",
      "Ep  253 | Reward:   0.89 | Steps:  31 | Eps: 0.382 | Loss: 0.0122\n",
      "Ep  254 | Reward:   0.93 | Steps:  20 | Eps: 0.381 | Loss: 0.0067\n",
      "Ep  255 | Reward:   0.94 | Steps:  18 | Eps: 0.381 | Loss: 0.0242\n",
      "Ep  256 | Reward:   0.92 | Steps:  24 | Eps: 0.380 | Loss: 0.0232\n",
      "Ep  257 | Reward:   0.91 | Steps:  25 | Eps: 0.379 | Loss: 0.0121\n",
      "Ep  258 | Reward:   0.92 | Steps:  23 | Eps: 0.378 | Loss: 0.0092\n",
      "Ep  259 | Reward:   0.94 | Steps:  16 | Eps: 0.377 | Loss: 0.0051\n",
      "Évaluation Ep 260: Reward: 0.96, Steps: 12.0\n",
      "Ep  260 | Reward:   0.94 | Steps:  17 | Eps: 0.377 | Loss: 0.0089\n",
      "Ep  261 | Reward:   0.85 | Steps:  44 | Eps: 0.375 | Loss: 0.0191\n",
      "Ep  262 | Reward:   0.91 | Steps:  27 | Eps: 0.374 | Loss: 0.0068\n",
      "Ep  263 | Reward:   0.94 | Steps:  16 | Eps: 0.374 | Loss: 0.0075\n",
      "Ep  264 | Reward:   0.93 | Steps:  19 | Eps: 0.373 | Loss: 0.0195\n",
      "Ep  265 | Reward:   0.94 | Steps:  16 | Eps: 0.372 | Loss: 0.0127\n",
      "Ep  266 | Reward:   0.93 | Steps:  20 | Eps: 0.372 | Loss: 0.0058\n",
      "Ep  267 | Reward:   0.93 | Steps:  19 | Eps: 0.371 | Loss: 0.0035\n",
      "Ep  268 | Reward:   0.94 | Steps:  16 | Eps: 0.370 | Loss: 0.0048\n",
      "Ep  269 | Reward:   0.89 | Steps:  30 | Eps: 0.369 | Loss: 0.0063\n",
      "Évaluation Ep 270: Reward: 0.96, Steps: 12.0\n",
      "Ep  270 | Reward:   0.90 | Steps:  29 | Eps: 0.368 | Loss: 0.0061\n",
      "Ep  271 | Reward:   0.91 | Steps:  25 | Eps: 0.367 | Loss: 0.0048\n",
      "Ep  272 | Reward:   0.95 | Steps:  13 | Eps: 0.367 | Loss: 0.0014\n",
      "Ep  273 | Reward:   0.95 | Steps:  15 | Eps: 0.366 | Loss: 0.0192\n",
      "Ep  274 | Reward:   0.94 | Steps:  17 | Eps: 0.366 | Loss: 0.0068\n",
      "Ep  275 | Reward:   0.95 | Steps:  15 | Eps: 0.365 | Loss: 0.0120\n",
      "Ep  276 | Reward:   0.95 | Steps:  14 | Eps: 0.365 | Loss: 0.0047\n",
      "Ep  277 | Reward:   0.94 | Steps:  17 | Eps: 0.364 | Loss: 0.0046\n",
      "Ep  278 | Reward:   0.95 | Steps:  13 | Eps: 0.364 | Loss: 0.0049\n",
      "Ep  279 | Reward:   0.92 | Steps:  23 | Eps: 0.363 | Loss: 0.0082\n",
      "Évaluation Ep 280: Reward: 0.96, Steps: 12.0\n",
      "Ep  280 | Reward:   0.94 | Steps:  17 | Eps: 0.362 | Loss: 0.0125\n",
      "Ep  281 | Reward:   0.92 | Steps:  23 | Eps: 0.361 | Loss: 0.0096\n",
      "Ep  282 | Reward:   0.93 | Steps:  20 | Eps: 0.361 | Loss: 0.0166\n",
      "Ep  283 | Reward:   0.93 | Steps:  19 | Eps: 0.360 | Loss: 0.0037\n",
      "Ep  284 | Reward:   0.94 | Steps:  17 | Eps: 0.359 | Loss: 0.0029\n",
      "Ep  285 | Reward:   0.94 | Steps:  16 | Eps: 0.359 | Loss: 0.0047\n",
      "Ep  286 | Reward:   0.95 | Steps:  13 | Eps: 0.358 | Loss: 0.0067\n",
      "Ep  287 | Reward:   0.94 | Steps:  17 | Eps: 0.358 | Loss: 0.0229\n",
      "Ep  288 | Reward:   0.93 | Steps:  20 | Eps: 0.357 | Loss: 0.0080\n",
      "Ep  289 | Reward:   0.92 | Steps:  22 | Eps: 0.356 | Loss: 0.0035\n",
      "Évaluation Ep 290: Reward: 0.96, Steps: 12.0\n",
      "Ep  290 | Reward:   0.95 | Steps:  15 | Eps: 0.356 | Loss: 0.0211\n",
      "Ep  291 | Reward:   0.95 | Steps:  14 | Eps: 0.355 | Loss: 0.0135\n",
      "Ep  292 | Reward:   0.91 | Steps:  26 | Eps: 0.354 | Loss: 0.0124\n",
      "Ep  293 | Reward:   0.95 | Steps:  15 | Eps: 0.354 | Loss: 0.0071\n",
      "Ep  294 | Reward:   0.93 | Steps:  21 | Eps: 0.353 | Loss: 0.0197\n",
      "Ep  295 | Reward:   0.94 | Steps:  16 | Eps: 0.353 | Loss: 0.0066\n",
      "Ep  296 | Reward:   0.93 | Steps:  20 | Eps: 0.352 | Loss: 0.0111\n",
      "Ep  297 | Reward:   0.91 | Steps:  25 | Eps: 0.351 | Loss: 0.0063\n",
      "Ep  298 | Reward:   0.93 | Steps:  21 | Eps: 0.350 | Loss: 0.0152\n",
      "Ep  299 | Reward:   0.93 | Steps:  20 | Eps: 0.350 | Loss: 0.0137\n",
      "Évaluation Ep 300: Reward: 0.96, Steps: 12.0\n",
      "Ep  300 | Reward:   0.92 | Steps:  22 | Eps: 0.349 | Loss: 0.0095\n",
      "Ep  301 | Reward:   0.91 | Steps:  26 | Eps: 0.348 | Loss: 0.0103\n",
      "Ep  302 | Reward:   0.92 | Steps:  24 | Eps: 0.347 | Loss: 0.0097\n",
      "Ep  303 | Reward:   0.91 | Steps:  25 | Eps: 0.346 | Loss: 0.0231\n",
      "Ep  304 | Reward:   0.94 | Steps:  18 | Eps: 0.346 | Loss: 0.0040\n",
      "Ep  305 | Reward:   0.92 | Steps:  24 | Eps: 0.345 | Loss: 0.0027\n",
      "Ep  306 | Reward:   0.95 | Steps:  15 | Eps: 0.345 | Loss: 0.0044\n",
      "Ep  307 | Reward:   0.93 | Steps:  19 | Eps: 0.344 | Loss: 0.0020\n",
      "Ep  308 | Reward:   0.90 | Steps:  28 | Eps: 0.343 | Loss: 0.0059\n",
      "Ep  309 | Reward:   0.91 | Steps:  25 | Eps: 0.342 | Loss: 0.0024\n",
      "Évaluation Ep 310: Reward: 0.96, Steps: 12.0\n",
      "Ep  310 | Reward:   0.94 | Steps:  16 | Eps: 0.342 | Loss: 0.0111\n",
      "Ep  311 | Reward:   0.94 | Steps:  17 | Eps: 0.341 | Loss: 0.0207\n",
      "Ep  312 | Reward:   0.93 | Steps:  20 | Eps: 0.340 | Loss: 0.0096\n",
      "Ep  313 | Reward:   0.87 | Steps:  38 | Eps: 0.339 | Loss: 0.0090\n",
      "Ep  314 | Reward:   0.93 | Steps:  21 | Eps: 0.338 | Loss: 0.0150\n",
      "Ep  315 | Reward:   0.93 | Steps:  20 | Eps: 0.338 | Loss: 0.0105\n",
      "Ep  316 | Reward:   0.94 | Steps:  17 | Eps: 0.337 | Loss: 0.0079\n",
      "Ep  317 | Reward:   0.94 | Steps:  17 | Eps: 0.337 | Loss: 0.0066\n",
      "Ep  318 | Reward:   0.95 | Steps:  15 | Eps: 0.336 | Loss: 0.0174\n",
      "Ep  319 | Reward:   0.93 | Steps:  19 | Eps: 0.336 | Loss: 0.0157\n",
      "Évaluation Ep 320: Reward: 0.96, Steps: 12.0\n",
      "Ep  320 | Reward:   0.94 | Steps:  17 | Eps: 0.335 | Loss: 0.0049\n",
      "Ep  321 | Reward:   0.90 | Steps:  28 | Eps: 0.334 | Loss: 0.0087\n",
      "Ep  322 | Reward:   0.94 | Steps:  17 | Eps: 0.334 | Loss: 0.0071\n",
      "Ep  323 | Reward:   0.93 | Steps:  21 | Eps: 0.333 | Loss: 0.0122\n",
      "Ep  324 | Reward:   0.92 | Steps:  23 | Eps: 0.332 | Loss: 0.0248\n",
      "Ep  325 | Reward:   0.93 | Steps:  19 | Eps: 0.332 | Loss: 0.0263\n",
      "Ep  326 | Reward:   0.95 | Steps:  14 | Eps: 0.331 | Loss: 0.0074\n",
      "Ep  327 | Reward:   0.94 | Steps:  17 | Eps: 0.331 | Loss: 0.0070\n",
      "Ep  328 | Reward:   0.95 | Steps:  15 | Eps: 0.330 | Loss: 0.0028\n",
      "Ep  329 | Reward:   0.95 | Steps:  15 | Eps: 0.330 | Loss: 0.0079\n",
      "Évaluation Ep 330: Reward: 0.96, Steps: 12.0\n",
      "Ep  330 | Reward:   0.94 | Steps:  18 | Eps: 0.329 | Loss: 0.0082\n",
      "Ep  331 | Reward:   0.92 | Steps:  23 | Eps: 0.328 | Loss: 0.0221\n",
      "Ep  332 | Reward:   0.95 | Steps:  14 | Eps: 0.328 | Loss: 0.0056\n",
      "Ep  333 | Reward:   0.92 | Steps:  24 | Eps: 0.327 | Loss: 0.0110\n",
      "Ep  334 | Reward:   0.92 | Steps:  23 | Eps: 0.326 | Loss: 0.0082\n",
      "Ep  335 | Reward:   0.94 | Steps:  18 | Eps: 0.326 | Loss: 0.0031\n",
      "Ep  336 | Reward:   0.96 | Steps:  12 | Eps: 0.325 | Loss: 0.0112\n",
      "Ep  337 | Reward:   0.93 | Steps:  21 | Eps: 0.325 | Loss: 0.0156\n",
      "Ep  338 | Reward:   0.93 | Steps:  19 | Eps: 0.324 | Loss: 0.0168\n",
      "Ep  339 | Reward:   0.93 | Steps:  20 | Eps: 0.323 | Loss: 0.0062\n",
      "Évaluation Ep 340: Reward: 0.96, Steps: 12.0\n",
      "Ep  340 | Reward:   0.94 | Steps:  16 | Eps: 0.323 | Loss: 0.0024\n",
      "Ep  341 | Reward:   0.94 | Steps:  16 | Eps: 0.322 | Loss: 0.0160\n",
      "Ep  342 | Reward:   0.95 | Steps:  13 | Eps: 0.322 | Loss: 0.0039\n",
      "Ep  343 | Reward:   0.93 | Steps:  19 | Eps: 0.321 | Loss: 0.0169\n",
      "Ep  344 | Reward:   0.95 | Steps:  14 | Eps: 0.321 | Loss: 0.0103\n",
      "Ep  345 | Reward:   0.94 | Steps:  16 | Eps: 0.321 | Loss: 0.0145\n",
      "Ep  346 | Reward:   0.92 | Steps:  22 | Eps: 0.320 | Loss: 0.0135\n",
      "Ep  347 | Reward:   0.92 | Steps:  22 | Eps: 0.319 | Loss: 0.0059\n",
      "Ep  348 | Reward:   0.94 | Steps:  16 | Eps: 0.319 | Loss: 0.0136\n",
      "Ep  349 | Reward:   0.95 | Steps:  13 | Eps: 0.318 | Loss: 0.0186\n",
      "Évaluation Ep 350: Reward: 0.96, Steps: 12.0\n",
      "Ep  350 | Reward:   0.94 | Steps:  18 | Eps: 0.318 | Loss: 0.0106\n",
      "Ep  351 | Reward:   0.94 | Steps:  18 | Eps: 0.317 | Loss: 0.0031\n",
      "Ep  352 | Reward:   0.92 | Steps:  24 | Eps: 0.316 | Loss: 0.0095\n",
      "Ep  353 | Reward:   0.94 | Steps:  16 | Eps: 0.316 | Loss: 0.0190\n",
      "Ep  354 | Reward:   0.95 | Steps:  14 | Eps: 0.316 | Loss: 0.0119\n",
      "Ep  355 | Reward:   0.95 | Steps:  14 | Eps: 0.315 | Loss: 0.0207\n",
      "Ep  356 | Reward:   0.93 | Steps:  19 | Eps: 0.315 | Loss: 0.0094\n",
      "Ep  357 | Reward:   0.92 | Steps:  22 | Eps: 0.314 | Loss: 0.0047\n",
      "Ep  358 | Reward:   0.94 | Steps:  17 | Eps: 0.313 | Loss: 0.0038\n",
      "Ep  359 | Reward:   0.95 | Steps:  14 | Eps: 0.313 | Loss: 0.0013\n",
      "Évaluation Ep 360: Reward: 0.96, Steps: 12.0\n",
      "Ep  360 | Reward:   0.95 | Steps:  15 | Eps: 0.312 | Loss: 0.0064\n",
      "Ep  361 | Reward:   0.95 | Steps:  14 | Eps: 0.312 | Loss: 0.0141\n",
      "Ep  362 | Reward:   0.93 | Steps:  19 | Eps: 0.311 | Loss: 0.0164\n",
      "Ep  363 | Reward:   0.94 | Steps:  17 | Eps: 0.311 | Loss: 0.0122\n",
      "Ep  364 | Reward:   0.94 | Steps:  16 | Eps: 0.310 | Loss: 0.0090\n",
      "Ep  365 | Reward:   0.92 | Steps:  23 | Eps: 0.310 | Loss: 0.0125\n",
      "Ep  366 | Reward:   0.90 | Steps:  29 | Eps: 0.309 | Loss: 0.0095\n",
      "Ep  367 | Reward:   0.95 | Steps:  15 | Eps: 0.308 | Loss: 0.0218\n",
      "Ep  368 | Reward:   0.94 | Steps:  16 | Eps: 0.308 | Loss: 0.0054\n",
      "Ep  369 | Reward:   0.93 | Steps:  20 | Eps: 0.307 | Loss: 0.0015\n",
      "Évaluation Ep 370: Reward: 0.96, Steps: 12.0\n",
      "Ep  370 | Reward:   0.94 | Steps:  17 | Eps: 0.307 | Loss: 0.0020\n",
      "Ep  371 | Reward:   0.92 | Steps:  22 | Eps: 0.306 | Loss: 0.0113\n",
      "Ep  372 | Reward:   0.94 | Steps:  17 | Eps: 0.306 | Loss: 0.0056\n",
      "Ep  373 | Reward:   0.92 | Steps:  22 | Eps: 0.305 | Loss: 0.0090\n",
      "Ep  374 | Reward:   0.95 | Steps:  15 | Eps: 0.305 | Loss: 0.0151\n",
      "Ep  375 | Reward:   0.94 | Steps:  18 | Eps: 0.304 | Loss: 0.0084\n",
      "Ep  376 | Reward:   0.94 | Steps:  17 | Eps: 0.304 | Loss: 0.0023\n",
      "Ep  377 | Reward:   0.95 | Steps:  14 | Eps: 0.303 | Loss: 0.0060\n",
      "Ep  378 | Reward:   0.95 | Steps:  14 | Eps: 0.303 | Loss: 0.0150\n",
      "Ep  379 | Reward:   0.95 | Steps:  14 | Eps: 0.302 | Loss: 0.0017\n",
      "Évaluation Ep 380: Reward: 0.96, Steps: 12.0\n",
      "Ep  380 | Reward:   0.94 | Steps:  17 | Eps: 0.302 | Loss: 0.0110\n",
      "Ep  381 | Reward:   0.94 | Steps:  16 | Eps: 0.301 | Loss: 0.0068\n",
      "Ep  382 | Reward:   0.92 | Steps:  23 | Eps: 0.301 | Loss: 0.0072\n",
      "Ep  383 | Reward:   0.92 | Steps:  24 | Eps: 0.300 | Loss: 0.0081\n",
      "Ep  384 | Reward:   0.94 | Steps:  16 | Eps: 0.300 | Loss: 0.0023\n",
      "Ep  385 | Reward:   0.92 | Steps:  22 | Eps: 0.299 | Loss: 0.0029\n",
      "Ep  386 | Reward:   0.94 | Steps:  16 | Eps: 0.298 | Loss: 0.0043\n",
      "Ep  387 | Reward:   0.94 | Steps:  17 | Eps: 0.298 | Loss: 0.0324\n",
      "Ep  388 | Reward:   0.95 | Steps:  15 | Eps: 0.298 | Loss: 0.0105\n",
      "Ep  389 | Reward:   0.95 | Steps:  13 | Eps: 0.297 | Loss: 0.0142\n",
      "Évaluation Ep 390: Reward: 0.96, Steps: 12.0\n",
      "Ep  390 | Reward:   0.94 | Steps:  18 | Eps: 0.297 | Loss: 0.0130\n",
      "Ep  391 | Reward:   0.93 | Steps:  19 | Eps: 0.296 | Loss: 0.0028\n",
      "Ep  392 | Reward:   0.94 | Steps:  18 | Eps: 0.296 | Loss: 0.0096\n",
      "Ep  393 | Reward:   0.93 | Steps:  21 | Eps: 0.295 | Loss: 0.0146\n",
      "Ep  394 | Reward:   0.91 | Steps:  25 | Eps: 0.294 | Loss: 0.0049\n",
      "Ep  395 | Reward:   0.94 | Steps:  18 | Eps: 0.294 | Loss: 0.0088\n",
      "Ep  396 | Reward:   0.94 | Steps:  18 | Eps: 0.293 | Loss: 0.0018\n",
      "Ep  397 | Reward:   0.95 | Steps:  15 | Eps: 0.293 | Loss: 0.0160\n",
      "Ep  398 | Reward:   0.91 | Steps:  25 | Eps: 0.292 | Loss: 0.0102\n",
      "Ep  399 | Reward:   0.93 | Steps:  20 | Eps: 0.292 | Loss: 0.0051\n",
      "Évaluation Ep 400: Reward: 0.96, Steps: 12.0\n",
      "Ep  400 | Reward:   0.95 | Steps:  14 | Eps: 0.291 | Loss: 0.0054\n",
      "Ep  401 | Reward:   0.93 | Steps:  19 | Eps: 0.291 | Loss: 0.0076\n",
      "Ep  402 | Reward:   0.94 | Steps:  17 | Eps: 0.290 | Loss: 0.0128\n",
      "Ep  403 | Reward:   0.94 | Steps:  18 | Eps: 0.290 | Loss: 0.0054\n",
      "Ep  404 | Reward:   0.94 | Steps:  17 | Eps: 0.289 | Loss: 0.0052\n",
      "Ep  405 | Reward:   0.92 | Steps:  24 | Eps: 0.289 | Loss: 0.0058\n",
      "Ep  406 | Reward:   0.94 | Steps:  18 | Eps: 0.288 | Loss: 0.0147\n",
      "Ep  407 | Reward:   0.93 | Steps:  19 | Eps: 0.287 | Loss: 0.0104\n",
      "Ep  408 | Reward:   0.92 | Steps:  23 | Eps: 0.287 | Loss: 0.0104\n",
      "Ep  409 | Reward:   0.93 | Steps:  20 | Eps: 0.286 | Loss: 0.0081\n",
      "Évaluation Ep 410: Reward: 0.96, Steps: 12.0\n",
      "Ep  410 | Reward:   0.93 | Steps:  19 | Eps: 0.286 | Loss: 0.0053\n",
      "Ep  411 | Reward:   0.94 | Steps:  18 | Eps: 0.285 | Loss: 0.0166\n",
      "Ep  412 | Reward:   0.94 | Steps:  16 | Eps: 0.285 | Loss: 0.0015\n",
      "Ep  413 | Reward:   0.91 | Steps:  26 | Eps: 0.284 | Loss: 0.0150\n",
      "Ep  414 | Reward:   0.95 | Steps:  13 | Eps: 0.284 | Loss: 0.0040\n",
      "Ep  415 | Reward:   0.93 | Steps:  19 | Eps: 0.283 | Loss: 0.0087\n",
      "Ep  416 | Reward:   0.92 | Steps:  23 | Eps: 0.283 | Loss: 0.0257\n",
      "Ep  417 | Reward:   0.95 | Steps:  13 | Eps: 0.282 | Loss: 0.0074\n",
      "Ep  418 | Reward:   0.93 | Steps:  21 | Eps: 0.282 | Loss: 0.0051\n",
      "Ep  419 | Reward:   0.91 | Steps:  25 | Eps: 0.281 | Loss: 0.0191\n",
      "Évaluation Ep 420: Reward: 0.96, Steps: 12.0\n",
      "Ep  420 | Reward:   0.95 | Steps:  15 | Eps: 0.281 | Loss: 0.0075\n",
      "Ep  421 | Reward:   0.94 | Steps:  18 | Eps: 0.280 | Loss: 0.0099\n",
      "Ep  422 | Reward:   0.94 | Steps:  17 | Eps: 0.280 | Loss: 0.0077\n",
      "Ep  423 | Reward:   0.95 | Steps:  15 | Eps: 0.279 | Loss: 0.0027\n",
      "Ep  424 | Reward:   0.95 | Steps:  14 | Eps: 0.279 | Loss: 0.0157\n",
      "Ep  425 | Reward:   0.95 | Steps:  13 | Eps: 0.279 | Loss: 0.0050\n",
      "Ep  426 | Reward:   0.94 | Steps:  16 | Eps: 0.278 | Loss: 0.0121\n",
      "Ep  427 | Reward:   0.95 | Steps:  14 | Eps: 0.278 | Loss: 0.0090\n",
      "Ep  428 | Reward:   0.94 | Steps:  16 | Eps: 0.277 | Loss: 0.0269\n",
      "Ep  429 | Reward:   0.95 | Steps:  15 | Eps: 0.277 | Loss: 0.0063\n",
      "Évaluation Ep 430: Reward: 0.96, Steps: 12.0\n",
      "Ep  430 | Reward:   0.95 | Steps:  15 | Eps: 0.277 | Loss: 0.0036\n",
      "Ep  431 | Reward:   0.95 | Steps:  15 | Eps: 0.276 | Loss: 0.0084\n",
      "Ep  432 | Reward:   0.94 | Steps:  16 | Eps: 0.276 | Loss: 0.0153\n",
      "Ep  433 | Reward:   0.95 | Steps:  13 | Eps: 0.275 | Loss: 0.0055\n",
      "Ep  434 | Reward:   0.94 | Steps:  16 | Eps: 0.275 | Loss: 0.0104\n",
      "Ep  435 | Reward:   0.94 | Steps:  18 | Eps: 0.274 | Loss: 0.0110\n",
      "Ep  436 | Reward:   0.91 | Steps:  25 | Eps: 0.274 | Loss: 0.0050\n",
      "Ep  437 | Reward:   0.94 | Steps:  16 | Eps: 0.273 | Loss: 0.0052\n",
      "Ep  438 | Reward:   0.94 | Steps:  16 | Eps: 0.273 | Loss: 0.0052\n",
      "Ep  439 | Reward:   0.95 | Steps:  15 | Eps: 0.273 | Loss: 0.0061\n",
      "Évaluation Ep 440: Reward: 0.96, Steps: 12.0\n",
      "Ep  440 | Reward:   0.95 | Steps:  15 | Eps: 0.272 | Loss: 0.0045\n",
      "Ep  441 | Reward:   0.94 | Steps:  16 | Eps: 0.272 | Loss: 0.0014\n",
      "Ep  442 | Reward:   0.93 | Steps:  19 | Eps: 0.271 | Loss: 0.0153\n",
      "Ep  443 | Reward:   0.94 | Steps:  18 | Eps: 0.271 | Loss: 0.0096\n",
      "Ep  444 | Reward:   0.94 | Steps:  18 | Eps: 0.270 | Loss: 0.0059\n",
      "Ep  445 | Reward:   0.95 | Steps:  15 | Eps: 0.270 | Loss: 0.0123\n",
      "Ep  446 | Reward:   0.93 | Steps:  20 | Eps: 0.269 | Loss: 0.0052\n",
      "Ep  447 | Reward:   0.95 | Steps:  13 | Eps: 0.269 | Loss: 0.0053\n",
      "Ep  448 | Reward:   0.92 | Steps:  22 | Eps: 0.268 | Loss: 0.0063\n",
      "Ep  449 | Reward:   0.94 | Steps:  18 | Eps: 0.268 | Loss: 0.0064\n",
      "Évaluation Ep 450: Reward: 0.96, Steps: 12.0\n",
      "Ep  450 | Reward:   0.96 | Steps:  12 | Eps: 0.268 | Loss: 0.0041\n",
      "Ep  451 | Reward:   0.92 | Steps:  24 | Eps: 0.267 | Loss: 0.0030\n",
      "Ep  452 | Reward:   0.95 | Steps:  14 | Eps: 0.267 | Loss: 0.0144\n",
      "Ep  453 | Reward:   0.95 | Steps:  15 | Eps: 0.266 | Loss: 0.0032\n",
      "Ep  454 | Reward:   0.91 | Steps:  25 | Eps: 0.266 | Loss: 0.0089\n",
      "Ep  455 | Reward:   0.92 | Steps:  22 | Eps: 0.265 | Loss: 0.0153\n",
      "Ep  456 | Reward:   0.92 | Steps:  23 | Eps: 0.265 | Loss: 0.0200\n",
      "Ep  457 | Reward:   0.95 | Steps:  13 | Eps: 0.264 | Loss: 0.0051\n",
      "Ep  458 | Reward:   0.94 | Steps:  17 | Eps: 0.264 | Loss: 0.0029\n",
      "Ep  459 | Reward:   0.94 | Steps:  16 | Eps: 0.263 | Loss: 0.0128\n",
      "Évaluation Ep 460: Reward: 0.96, Steps: 12.0\n",
      "Ep  460 | Reward:   0.94 | Steps:  17 | Eps: 0.263 | Loss: 0.0045\n",
      "Ep  461 | Reward:   0.93 | Steps:  19 | Eps: 0.262 | Loss: 0.0102\n",
      "Ep  462 | Reward:   0.94 | Steps:  16 | Eps: 0.262 | Loss: 0.0016\n",
      "Ep  463 | Reward:   0.92 | Steps:  23 | Eps: 0.261 | Loss: 0.0038\n",
      "Ep  464 | Reward:   0.94 | Steps:  16 | Eps: 0.261 | Loss: 0.0029\n",
      "Ep  465 | Reward:   0.94 | Steps:  16 | Eps: 0.261 | Loss: 0.0042\n",
      "Ep  466 | Reward:   0.92 | Steps:  22 | Eps: 0.260 | Loss: 0.0074\n",
      "Ep  467 | Reward:   0.95 | Steps:  14 | Eps: 0.260 | Loss: 0.0221\n",
      "Ep  468 | Reward:   0.93 | Steps:  20 | Eps: 0.259 | Loss: 0.0087\n",
      "Ep  469 | Reward:   0.92 | Steps:  22 | Eps: 0.259 | Loss: 0.0085\n",
      "Évaluation Ep 470: Reward: 0.96, Steps: 12.0\n",
      "Ep  470 | Reward:   0.94 | Steps:  16 | Eps: 0.258 | Loss: 0.0052\n",
      "Ep  471 | Reward:   0.95 | Steps:  14 | Eps: 0.258 | Loss: 0.0043\n",
      "Ep  472 | Reward:   0.94 | Steps:  17 | Eps: 0.258 | Loss: 0.0067\n",
      "Ep  473 | Reward:   0.94 | Steps:  18 | Eps: 0.257 | Loss: 0.0110\n",
      "Ep  474 | Reward:   0.94 | Steps:  16 | Eps: 0.257 | Loss: 0.0126\n",
      "Ep  475 | Reward:   0.95 | Steps:  14 | Eps: 0.256 | Loss: 0.0109\n",
      "Ep  476 | Reward:   0.94 | Steps:  16 | Eps: 0.256 | Loss: 0.0137\n",
      "Ep  477 | Reward:   0.95 | Steps:  15 | Eps: 0.256 | Loss: 0.0148\n",
      "Ep  478 | Reward:   0.94 | Steps:  18 | Eps: 0.255 | Loss: 0.0098\n",
      "Ep  479 | Reward:   0.95 | Steps:  13 | Eps: 0.255 | Loss: 0.0068\n",
      "Évaluation Ep 480: Reward: 0.96, Steps: 12.0\n",
      "Ep  480 | Reward:   0.95 | Steps:  15 | Eps: 0.254 | Loss: 0.0143\n",
      "Ep  481 | Reward:   0.94 | Steps:  17 | Eps: 0.254 | Loss: 0.0206\n",
      "Ep  482 | Reward:   0.95 | Steps:  15 | Eps: 0.254 | Loss: 0.0068\n",
      "Ep  483 | Reward:   0.93 | Steps:  20 | Eps: 0.253 | Loss: 0.0124\n",
      "Ep  484 | Reward:   0.94 | Steps:  18 | Eps: 0.253 | Loss: 0.0064\n",
      "Ep  485 | Reward:   0.94 | Steps:  16 | Eps: 0.252 | Loss: 0.0082\n",
      "Ep  486 | Reward:   0.93 | Steps:  20 | Eps: 0.252 | Loss: 0.0078\n",
      "Ep  487 | Reward:   0.94 | Steps:  16 | Eps: 0.252 | Loss: 0.0064\n",
      "Ep  488 | Reward:   0.93 | Steps:  20 | Eps: 0.251 | Loss: 0.0084\n",
      "Ep  489 | Reward:   0.95 | Steps:  14 | Eps: 0.251 | Loss: 0.0144\n",
      "Évaluation Ep 490: Reward: 0.96, Steps: 12.0\n",
      "Ep  490 | Reward:   0.94 | Steps:  16 | Eps: 0.250 | Loss: 0.0138\n",
      "Ep  491 | Reward:   0.95 | Steps:  13 | Eps: 0.250 | Loss: 0.0055\n",
      "Ep  492 | Reward:   0.95 | Steps:  15 | Eps: 0.250 | Loss: 0.0129\n",
      "Ep  493 | Reward:   0.93 | Steps:  21 | Eps: 0.249 | Loss: 0.0027\n",
      "Ep  494 | Reward:   0.95 | Steps:  15 | Eps: 0.249 | Loss: 0.0097\n",
      "Ep  495 | Reward:   0.94 | Steps:  16 | Eps: 0.248 | Loss: 0.0031\n",
      "Ep  496 | Reward:   0.94 | Steps:  18 | Eps: 0.248 | Loss: 0.0048\n",
      "Ep  497 | Reward:   0.95 | Steps:  14 | Eps: 0.248 | Loss: 0.0014\n",
      "Ep  498 | Reward:   0.95 | Steps:  14 | Eps: 0.247 | Loss: 0.0021\n",
      "Ep  499 | Reward:   0.94 | Steps:  16 | Eps: 0.247 | Loss: 0.0089\n",
      "Évaluation Ep 500: Reward: 0.96, Steps: 12.0\n",
      "Ep  500 | Reward:   0.94 | Steps:  16 | Eps: 0.247 | Loss: 0.0050\n",
      "Vidéo sauvegardée : videos/minigrid_video.mp4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Démarrage de l'entraînement DQN optimisé...\")\n",
    "    trained_model = train_agent(num_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5b5f8",
   "metadata": {
    "papermill": {
     "duration": 0.021498,
     "end_time": "2025-05-10T23:37:23.356201",
     "exception": false,
     "start_time": "2025-05-10T23:37:23.334703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 457.609602,
   "end_time": "2025-05-10T23:37:26.930057",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T23:29:49.320455",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
